{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ______________________________________________Begin_____________________________________________________\n",
      "\n",
      " Processing File: \"data/BROWN.pos.all\"\n",
      "\n",
      "\n",
      "______________________________________________Ending Taining_____________________________________________\n",
      "\n",
      "\n",
      "______________________________________________Begin_____________________________________________________\n",
      "\n",
      " Processing File: \"data/SnapshotBROWN.pos.all.txt\"\n",
      "\n",
      "\n",
      "     Performance Report of File - \"data/SnapshotBROWN.pos.all.txt\"  :  \n",
      " ___________________________________________________________________\n",
      " ***********************************************************************************\n",
      "\n",
      "     Accuracy Percentile                    : 91.29%\n",
      "     Error Percentile                       : 8.71%\n",
      "     Unspecified Word in Tagset(percentile) : 0.00%\n",
      "\n",
      " ***********************************************************************************\n",
      "\n",
      " ______________________________________________END_____________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "-------------------------Some taggging for new text shown below--------------------\n",
      "-----------------------------------------------------------------------------------\n",
      "Word (New-Tagged)        :   NNS   Researchers\n",
      "Word (New-Tagged)        :    DT   The\n",
      "Word (Known-Tagged)      :    VB   turn\n",
      "Word (Known-Tagged)      :  PRP$   our\n",
      "Word (New-Tagged)        :   NNP   But\n",
      "Word (Known-Tagged)      :    NN   fear\n",
      "Word (New-Tagged)        :   NNP   In\n",
      "Word (Known-Tagged)      :   PRP   them\n",
      "Word (New-Not Tagged)    : <NONE>  That\n",
      "Word (New-Tagged)        :   NNP   Institute\n",
      "Word (New-Tagged)        :     \"   \"\n",
      "Word (Known-Tagged)      :   NNS   computers\n",
      "Word (New-Tagged)        :     \"   \"\n",
      "Word (Known-Tagged)      :    VB   be\n",
      "Word (New-Tagged)        :    DT   The\n",
      "Word (Known-Tagged)      :   VBZ   allows\n",
      "Word (New-Not Tagged)    : <NONE>  Think\n",
      "Word (New-Tagged)        :     \"   \"\n",
      "Word (Known-Tagged)      :    NN   search\n",
      "Word (New-Tagged)        :    DT   The\n",
      "Word (New-Tagged)        :   NNP   Tom\n",
      "Word (Known-Tagged)      :    CD   21\n",
      "Word (New-Tagged)        :   NNP   Criteria\n",
      "Word (New-Not Tagged)    : <NONE>  online\n",
      "Word (New-Tagged)        :   NNP   Mitchell\n",
      "Word (Known-Tagged)      :   NNP   s\n",
      "Word (New-Tagged)        :    DT   The\n",
      "Word (Known-Tagged)      :    JJ   available\n",
      "Word (New-Not Tagged)    : <NONE>  That\n",
      "Word (Known-Tagged)      :    NN   time\n",
      "Word (New-Tagged)        :     \"   \"\n",
      "Word (Known-Tagged)      :    NN   ground\n",
      "Word (New-Tagged)        :     )   ]\n",
      "Word (Known-Tagged)      :    IN   up\n",
      "Word (New-Tagged)        :   NNP   Mitchell\n",
      "Word (New-Tagged)        :    BB   Essentially\n",
      "Word (Known-Tagged)      :    CC   and\n",
      "Word (New-Tagged)        :   NNP   Facebook\n",
      "Word (New-Tagged)        :   NNP   AI\n",
      "Word (New-Tagged)        :   NNS   Leaders\n",
      "Word (New-Tagged)        :   NNP   San\n",
      "Word (New-Tagged)        :   NNS   Robots\n",
      "Word (Known-Tagged)      :   PRP   it\n",
      "Word (New-Tagged)        :   NNP   At\n",
      "Word (New-Tagged)        :   NNS   News\n",
      "Word (Known-Tagged)      :     ,   ,\n",
      "Word (New-Tagged)        :   NNP   Candela\n",
      "Word (Known-Tagged)      :     .   .\n",
      "Word (New-Tagged)        :   NNP   Airbnb\n",
      "Word (New-Tagged)        :   NNP   Even\n",
      "Word (New-Tagged)        :   NNS   printers\n",
      "Word (Known-Tagged)      :    DT   the\n",
      "Word (New-Tagged)        :   NNP   Subtle\n",
      "Word (Known-Tagged)      :    DT   the\n",
      "Word (New-Tagged)        :   VBN   Indeed\n",
      "Word (Known-Tagged)      :    DT   the\n",
      "Word (New-Tagged)        :    DT   The\n",
      "Word (Known-Tagged)      :    MD   will\n",
      "Word (New-Tagged)        :   VBG   According\n",
      "Word (New-Not Tagged)    : <NONE>  S\n",
      "Word (Known-Tagged)      :    DT   the\n",
      "Word (Known-Tagged)      :    IN   of\n",
      " _____________________________________________________________________________________________\n",
      "\n",
      "______________________________________________Begin_____________________________________________________\n",
      "\n",
      "\n",
      "                           Performance Report of New Content                                              \n",
      " ___________________________________________________________________________________________________________\n",
      " *********************************************************************************************************\n",
      "\n",
      " Total Number of Words                          : 844         \n",
      " Tagged Words Known (percentile among all words): 716 (84.83%)\n",
      " New Words(percentile among all words)          : 128 (15.17%\n",
      ")\n",
      " Words Tagged(percentile among all new words)   : 113 (88.28%\n",
      ")\n",
      " Words Could Not Tag (percentile in new words)  : 15 (11.72%\n",
      ")\n",
      " *********************************************************************************************************\n",
      "\n",
      " ______________________________________________END_____________________________________________________\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "'''\n",
    "* Author           : Anjana Tiha\n",
    "* Assignment No    : #3 \n",
    "* Course           : Natural Language Processing (COMP 8780)\n",
    "* Semester         : Spring 2018\n",
    "* Course Instructor: Professor Visali Rus\n",
    "* University       : University of Memphis \n",
    "* Deadline         : Due Mar. 1, 2018.\n",
    "*\n",
    "* Description      : 1. Builds a baseline statistical tagger by using the assignment#2's hash of hashes.\n",
    "*                    2. Train baseline lexicalized statistical tagger on the entire BROWN corpus.\n",
    "*                    3. Uses the baseline lexicalized statistical tagger to tag all the words in the SnapshotBROWN.pos.all.txt file.\n",
    "*                    4. Evaluates and reports the performance of this baseline tagger on the Snapshot file.\n",
    "*                    5. Adds rules for unknown word tagging.\n",
    "*                    6. Tests on new text collected from article.\n",
    "*\n",
    "* Description      : 1. Maps each parse tree in the BROWN.pos.all file into one-line sentences.\n",
    "* (Detailed)         2. Each sentence span a single line in the output file.\n",
    "*                    3. Generates the hash of hashes from the clean file BROWN-clean.pos.txt in word:pos:freq format. \n",
    "*                    4. Takes the most frequent tag and use it to tag the words in all the sentences from the SnapshotBROWN-clean.pos.txt file. \n",
    "*                    5. Report the performance (Accuracy, error, percentile not prensent in tagset) of this tagger.\n",
    "*\n",
    "* Comments         : Please use Anaconda editor for convenience.\n",
    "* Tools Requirement: Anaconda, Python \n",
    "* Current Version  : v1.0.2.1\n",
    "* Version History  : v1.0.0.0.0 (Coppied Assignment 2)\n",
    "*                  : v1.0.0.0.1 (Completed lexicalized statistical tagger for full corpus and tested on snapshot)\n",
    "*                  : v1.0.4.2 (Completed adding unknown word rules and tested on new text)      \n",
    "* Last Update      : 02.28.2018 (Time : 06:22am)\n",
    "*\n",
    "'''         \n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "'''\n",
    "Question \n",
    "\n",
    "Assignment #3: Due March 1.\n",
    "-----------------------------------------------------------------------\n",
    "Instructor Instruction:\n",
    "\n",
    "Remember that the homework is due by midnight on the due date. You must \n",
    "turn in a soft copy via email to TA. Your submission should have a cover page \n",
    "and one or more summary pages where you provide for each problem the\n",
    "answer. You should not submit the data you use if the data is too\n",
    "large. You must submit your code.\n",
    "\n",
    "\n",
    "1. Build a baseline statistical tagger.\n",
    "\n",
    "(i) [10 points] Use the assignment#2's hash of hashes to train a\n",
    "baseline lexicalized statistical tagger on the entire BROWN corpus.\n",
    "\n",
    "(ii) [20 points] Use the baseline lexicalized statistical tagger to tag \n",
    "all the words in the SnapshotBROWN.pos.all.txt file. Evaluate and report the\n",
    "performance of this baseline tagger on the Snapshot file.\n",
    "\n",
    "(iii) [20 points] add few rules to handle unknown words for the tagger\n",
    "   in (ii). The rules can be morphological, contextual, or of other\n",
    "   nature. Use 25 new sentences to evaluate this tagger (the (ii) tagger +\n",
    "   unknown word rules). You can pick 25 sentences from a news article\n",
    "   from the web and report the performance on those.\n",
    "\n",
    "NOTE: You should only consider the 45 proper tags from Penn Treebank\n",
    "tagset (available in the slides). You should disregard tags such as\n",
    "-NONE-, etc.\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "import re\n",
    "import os\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# removes file\n",
    "def remove_file(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "        return 1\n",
    "    except OSError:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# reads file\n",
    "def read_file(filename):             \n",
    "    with open(filename, 'r', encoding=\"utf8\") as content_file:\n",
    "        content = content_file.read()\n",
    "    return content\n",
    "\n",
    "\n",
    "# read file line by line/ splits by line\n",
    "def read_file_line_by_line(filename):             \n",
    "    with open(filename, 'r', encoding=\"utf8\") as content_file:\n",
    "        content = content_file.readlines()\n",
    "    return content\n",
    "\n",
    "\n",
    "# writes file whole content\n",
    "def write(filename, content):\n",
    "    fh = open(filename,\"w+\", encoding=\"utf8\")\n",
    "    fh.write(content)\n",
    "    fh.close()\n",
    "    return filename\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# print file in one pass\n",
    "def print_complete_file(filename):             \n",
    "    with open(filename, 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "    return content\n",
    "\n",
    "\n",
    "# print file line by line/ splits by line\n",
    "def print_file_line_by_line(filename):             \n",
    "    with open(filename, 'r') as content_file:\n",
    "        content = content_file.readlines()\n",
    "    return content\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "# prints dictionary\n",
    "def print_dict(dict_s):\n",
    "    for i in dict_s:\n",
    "        print(i)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# converts text to lowercase\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "# removes all spaces \n",
    "# replaces \" \" with \"\"\n",
    "def remove_all_space(text):\n",
    "    return re.sub(r' +', '', text)\n",
    "\n",
    "# removes multiple spaces with single space\n",
    "def remove_multi_space(text):\n",
    "    return re.sub(r' +', ' ', text)\n",
    "\n",
    "\n",
    "# add space before and after the punctuation\n",
    "def add_space_punc(text):\n",
    "    return re.sub(\"([^a-zA-Z0-9])\", r' \\1 ', text)\n",
    "\n",
    "# remove all the characters except alphabetical\n",
    "# removes special characters and numerical charcharters\n",
    "def remove_non_alpha(text):\n",
    "    return re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "\n",
    "\n",
    "# check if substring present in text\n",
    "def text_contains(text, substr):\n",
    "    if(substr in text): \n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# splits stiong by space or \" \"\n",
    "def split_string(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "# Removes all blank lines\n",
    "def remove_extra_blank_lines(content):   \n",
    "    return re.sub(r'\\n\\s*\\n', '\\n', content)\n",
    "    return txt\n",
    "\n",
    "\n",
    "\n",
    "# get word frequency from word list\n",
    "def get_word_freq(word_tokens):\n",
    "    word_freq = {}\n",
    "    for w in word_tokens:\n",
    "        if w in word_freq:\n",
    "            word_freq[w] +=1\n",
    "        else:\n",
    "            word_freq[w] = 1\n",
    "    return word_freq\n",
    "\n",
    "\n",
    "# sort dictionary by key or value\n",
    "def sort_dict(dict_x, type_s):\n",
    "    if type_s == \"key\":\n",
    "        return sorted(dict_x.items(), key=operator.itemgetter(0))\n",
    "    elif type_s == \"val\":\n",
    "        return OrderedDict(sorted(dict_x.items(), key=operator.itemgetter(1), reverse=True))\n",
    "    else:\n",
    "        return OrderedDict(sorted(dict_x.items(), key=operator.itemgetter(0), reverse=True))\n",
    "    \n",
    "# prints first num number of dictionary key and valye pair\n",
    "def print_most_dict(dict_map, num):\n",
    "    count = 0\n",
    "    print(\"*********************************************\")\n",
    "    for tag in dict_map:\n",
    "        print(tag, \" : \", dict_map[tag])\n",
    "        count +=1\n",
    "        if count >= num:\n",
    "            break\n",
    "    print(\"*********************************************\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# read annotated file with pos\n",
    "# clean file and save text in pos: word format \n",
    "def get_pos_word(text, output_file):\n",
    "    blacklist = [\"-NONE-\", \"-LRB-\", \"-RRB-\"]  \n",
    "    eof_txt = \"(TOP END_OF_TEXT_UNIT)\"\n",
    "    i = 0\n",
    "    current_line = \"\"\n",
    "    remove_file(output_file)\n",
    "    \n",
    "    with open(output_file, 'a') as the_file:\n",
    "        for line in text:\n",
    "            if eof_txt in line:\n",
    "                current_line = current_line.strip()\n",
    "                if current_line != \"\":\n",
    "                    the_file.write(current_line + \"\\n\")\n",
    "                current_line = \"\"\n",
    "                continue\n",
    "            else:\n",
    "                temp = line.rsplit('(')\n",
    "                temp = temp[len(temp)-1] \n",
    "                temp = temp.rsplit(')')\n",
    "                temp = temp[0]\n",
    "                #temp = temp.strip()\n",
    "                pass_iter = 0\n",
    "                for item in blacklist:\n",
    "                    if item in temp:\n",
    "                        pass_iter = 1\n",
    "                        break\n",
    "                if pass_iter == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    #temp_test = re.sub(r'[^a-zA-Z0-9]', ' ', temp) \n",
    "                    #temp_test = remove_all_space(temp_test)\n",
    "                    temp_test  = temp\n",
    "                    if temp_test == \"\":\n",
    "                        continue\n",
    "                    else:\n",
    "                        temp_vocab = temp.split() \n",
    "                        if len(temp_vocab) == 2:\n",
    "                            temp = temp.strip()\n",
    "                            current_line += temp + \" \"  \n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "\n",
    "# get pos and frequency of each word in a document map={word:{pos:freq}} format\n",
    "def get_has_word_pos_freq(text):\n",
    "    pos = \"\"\n",
    "    word_pos = {}\n",
    "    curr_pos_word_freq = {}\n",
    "    for line in text:\n",
    "        line = line.strip()\n",
    "        temp = line.split(\" \")\n",
    "        for i in range(len(temp)):\n",
    "            if temp[i] == \"\\n\":\n",
    "                pos = \"\"\n",
    "                continue\n",
    "            if i%2 == 0:\n",
    "                pos = temp[i]\n",
    "            else:\n",
    "                vocab = temp[i]\n",
    "                vocab = to_lower(vocab)\n",
    "                if vocab in word_pos:\n",
    "                    curr_pos_word_freq = word_pos[vocab]\n",
    "                    if pos in curr_pos_word_freq:\n",
    "                        curr_pos_word_freq[pos] = curr_pos_word_freq[pos] + 1 \n",
    "                    else:\n",
    "                        curr_pos_word_freq[pos] = 1\n",
    "                else:\n",
    "                    curr_pos_word_freq[pos] = 1\n",
    "                    word_pos[vocab] = curr_pos_word_freq\n",
    "\n",
    "                curr_pos_word_freq = {}   \n",
    "    return word_pos\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# get frequency of each tag\n",
    "def get_tag_freq(word_pos):\n",
    "    tag_dict_freq = {}\n",
    "    for word in word_pos:\n",
    "        word_tags = word_pos[word]\n",
    "        for tag in word_tags:\n",
    "            if tag in tag_dict_freq:\n",
    "                tag_dict_freq[tag] = tag_dict_freq[tag] + word_tags[tag]\n",
    "            else:\n",
    "                tag_dict_freq[tag] = word_tags[tag]\n",
    "    return tag_dict_freq\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "\n",
    "# set maximum frequent pos to word\n",
    "def set_max_pos(word_pos):\n",
    "    word_pos_ch = {}\n",
    "\n",
    "    for word in word_pos:\n",
    "        cur_pos = \"\"\n",
    "        curr_word_pos_freq = 0\n",
    "        curr_word_pos = word_pos[word]\n",
    "        for pos in curr_word_pos:\n",
    "            if curr_word_pos_freq < curr_word_pos[pos]:\n",
    "                curr_word_pos_freq = curr_word_pos[pos]\n",
    "                cur_pos = pos\n",
    "        word_pos_ch[word] = cur_pos\n",
    "    return word_pos_ch\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "\n",
    "# get accuracy of adjusted tag in tagged text\n",
    "def get_tagger_performance(text, word_tag_max):\n",
    "    pos = \"\"\n",
    "    total_count = 0\n",
    "    correct_tag_count = 0\n",
    "    word_unspecified = 0\n",
    "    for line in text:\n",
    "        line = line.strip()\n",
    "        temp = line.split(\" \")\n",
    "        for i in range(len(temp)):\n",
    "            if temp[i] == \"\\n\":\n",
    "                pos = \"\"\n",
    "                continue\n",
    "            if i%2 == 0:\n",
    "                pos = temp[i]\n",
    "            else:\n",
    "                vocab = temp[i]\n",
    "                vocab = to_lower(vocab)\n",
    "                if vocab in word_tag_max:\n",
    "                    curr_pos_word = word_tag_max[vocab]\n",
    "                    if pos == curr_pos_word:\n",
    "                        correct_tag_count +=1 \n",
    "                    total_count += 1\n",
    "                else:\n",
    "                    word_unspecified +=1\n",
    "\n",
    "    return total_count, correct_tag_count, word_unspecified\n",
    "\n",
    "# calculate accuracy\n",
    "# calculate error, and percentile not present in tagset for tagging\n",
    "def accuracy_tag(total_count, correct_tag_count, word_unspecified):\n",
    "    accuracy = (float(correct_tag_count) / float(total_count)) * 100\n",
    "    error = 100 - accuracy\n",
    "    word_unspecified_percentile = (float(word_unspecified) / float(total_count)) * 100\n",
    "    return accuracy, error, word_unspecified_percentile\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "# all the task in assignment 2\n",
    "def assignment2(input_file, output_file, num):\n",
    "    print(\"\\n ______________________________________________Begin_____________________________________________________\\n\")\n",
    "    print(\" Processing File:  \\\"\", input_file, \"\\\" ..............................\\n\")\n",
    "    text = read_file_line_by_line(input_file)\n",
    "    get_pos_word(text, output_file)\n",
    "    text = read_file_line_by_line(output_file)\n",
    "    word_pos = get_has_word_pos_freq(text)\n",
    "    tag_dict_freq = get_tag_freq(word_pos)\n",
    "    tag_dict_freq = sort_dict(tag_dict_freq, \"val\")\n",
    "    \n",
    "    print(\" Most Frequent 20 Tags/POS of File - \", input_file, \" : \")\n",
    "    print(\" ___________________________________________________________________\")\n",
    "    print_most_dict(tag_dict_freq, num)\n",
    "    word_tag_max = set_max_pos(word_pos)\n",
    "    total_count, correct_tag_count, word_unspecified = get_tagger_performance(text, word_tag_max)\n",
    "    accuracy, error, word_unspecified_percentile = accuracy_tag(total_count, correct_tag_count, word_unspecified)\n",
    "    print(\"\\n     Performance Report of File - %s %s \"% (test_file, \" : \"))\n",
    "    print(\" ___________________________________________________________________\")\n",
    "    print(\" ***********************************************************************************\")\n",
    "    print(\"     Accuracy Percentile                    : %s%s\" % (accuracy,\"%\"))\n",
    "    print(\"     Error Percentile                       : %s%s\" % (error,\"%\"))\n",
    "    print(\"     Unspecified Word in Tagset(percentile) : %s%s\" % (word_unspecified_percentile,\"%\"))\n",
    "    print(\" ***********************************************************************************\")\n",
    "    print(\"\\n ______________________________________________END_____________________________________________________\\n\\n\\n\")\n",
    "    return word_tag_max\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "\n",
    "# baseline_lexical_tagger trained on full brown corpus and tested on snapshot brown corpus \n",
    "def baseline_lexical_tagger(input_file, output_file, test_file, test_file_out):\n",
    "    print(\"\\n ______________________________________________Begin_____________________________________________________\\n\")\n",
    "    print(\" Processing File: \\\"%s\\\"%s\" % (input_file, \"\\n\"))\n",
    "    text = read_file_line_by_line(input_file)\n",
    "    get_pos_word(text, output_file)\n",
    "    text = read_file_line_by_line(output_file)\n",
    "    word_pos = get_has_word_pos_freq(text)\n",
    "    word_tag_max = set_max_pos(word_pos)\n",
    "    print(\"\\n______________________________________________Ending Taining_____________________________________________\\n\")\n",
    "    print(\"\\n______________________________________________Begin_____________________________________________________\\n\")\n",
    "    print(\" Processing File: \\\"%s\\\"%s\" % (test_file, \"\\n\"))\n",
    "    text = read_file_line_by_line(test_file)\n",
    "    get_pos_word(text, test_file_out)\n",
    "    text = read_file_line_by_line(test_file_out)\n",
    "    total_count, correct_tag_count, word_unspecified = get_tagger_performance(text, word_tag_max)\n",
    "    accuracy, error, word_unspecified_percentile = accuracy_tag(total_count, correct_tag_count, word_unspecified)\n",
    "    print(\"\\n     Performance Report of File - \\\"%s\\\" %s \"% (test_file, \" : \"))\n",
    "    print(\" ___________________________________________________________________\")\n",
    "    print(\" ***********************************************************************************\\n\")\n",
    "    print(\"     Accuracy Percentile                    : %.2f%s\" % (accuracy,\"%\"))\n",
    "    print(\"     Error Percentile                       : %.2f%s\" % (error,\"%\"))\n",
    "    print(\"     Unspecified Word in Tagset(percentile) : %.2f%s\" % (word_unspecified_percentile,\"%\\n\"))\n",
    "    print(\" ***********************************************************************************\")\n",
    "    print(\"\\n ______________________________________________END_____________________________________________________\\n\\n\\n\")\n",
    "    return word_tag_max\n",
    "\n",
    "\n",
    "# cleans input file and rewrites whole file\n",
    "# rempoves extra empty lines\n",
    "def clean_file(filename):\n",
    "    content = read_file(filename)\n",
    "    content = remove_extra_blank_lines(content)\n",
    "    filename = write(filename, content)\n",
    "    content = read_file_line_by_line(filename)\n",
    "    return filename, content\n",
    "\n",
    "\n",
    "# Preprocesses content to seperate words and punctuations\n",
    "def preprocess_content(content, IGNORE_):\n",
    "    count = 0\n",
    "    content_modified = \"\"\n",
    "    for line in content:\n",
    "        line = line.strip()\n",
    "        if line not in IGNORE_:\n",
    "            count +=1\n",
    "            line = add_space_punc(line)\n",
    "            line = remove_multi_space(line)\n",
    "            content_modified = content_modified + line  + \"\\n\" \n",
    "        else:\n",
    "            pass\n",
    "    return content_modified\n",
    "\n",
    "\n",
    "# tags unknown words for supplementing baseline lexical tagger\n",
    "def tag_unknown_words(line, pos, word, word_tag_max):\n",
    "    text = word.strip()\n",
    "    \n",
    "    modals = [\"can\", \"could\", \"may\", \"might\", \"will\", \"would\", \"shall\", \"should\", \"must\"]\n",
    "    wh_determiner = [\"what\", \"which\", \"whose\", \"whatever\", \"whichever\"]\n",
    "    articles = [\"a\", \"an\", \"the\"]\n",
    "    personal_pronoun = [\"I\",\"me\", \"you\", \"he\", \"him\", \"she\", \"her\", \"it\", \"we\", \"us\", \"you\", \"they\", \"them\"]\n",
    "    possessive_pronoun = [\"mine\", \"yours\", \"his\", \"hers\", \"ours\", \"yours\", \"theirs\"]\n",
    "    \n",
    "    if text.lower() in modals:\n",
    "        return \"MD\"\n",
    "    elif text.lower() in wh_determiner:\n",
    "        return \"WDT\"\n",
    "    elif text.lower() in articles:\n",
    "        return \"DT\"\n",
    "    elif text.lower() in personal_pronoun:\n",
    "        return \"PP\"\n",
    "    elif text.lower() in possessive_pronoun:\n",
    "        return \"PP$\"\n",
    "    elif text[len(text)-2:] == \"ss\":\n",
    "        return \"NN\"\n",
    "    elif text[len(text)-2:] == \"ed\":\n",
    "        return \"VBN\"\n",
    "    elif text[len(text)-3:] == \"ing\":\n",
    "        return \"VBG\"\n",
    "    elif text[len(text)-2:] == \"ly\":\n",
    "        return \"BB\"\n",
    "    elif text+\"ly\" in word_tag_max:\n",
    "        return \"JJ\"\n",
    "    elif text[len(text)-2:] == \"us\":\n",
    "        return \"JJ\"\n",
    "    elif text[len(text)-3:] == \"ble\":\n",
    "        return \"JJ\"\n",
    "    elif text[len(text)-2:] == \"ic\":\n",
    "        return \"JJ\"\n",
    "    elif ((text[:2] == \"un\") and (text[2:] in word_tag_max)):\n",
    "        return \"JJ\"\n",
    "    elif text[len(text)-3:] == \"ive\":\n",
    "        return \"JJ\"\n",
    "    elif text[len(text)-1:] == \"s\":\n",
    "        return \"NNS\"\n",
    "    elif text.isdigit():\n",
    "        return \"CD\"\n",
    "    elif text_contains(text, \".\") and (text.strip()!=\".\"):\n",
    "        return \"CD\"\n",
    "    elif text_contains(text,\"-\") and (text.strip()!=\"-\"):\n",
    "        return \"JJ\"\n",
    "    elif text == \"+\" or text == \"%\" or text == \"&\":\n",
    "        return \"SYM\"\n",
    "    elif text == \"{\" or text == \"(\" or text == \"[\" or text == \"<\":\n",
    "        return \"(\"\n",
    "    elif text == \"}\" or text == \")\" or text == \"]\" or text == \">\":\n",
    "        return \")\"\n",
    "    elif text == \",\" or text == \";\" or text == \"-\" or text == \"-\":\n",
    "        return \",\"\n",
    "    elif text == \".\" or text == \"!\" or text == \"?\":\n",
    "        return \".\"\n",
    "    elif text == '$' or text == '#' or text == ',':\n",
    "        return text\n",
    "    elif len(text) == 1 and (text == \"\\\"\" or text == \"\\\"\" or text == \"'\" or text == \"'\" or text == '`' or text == '\"\"' or text == \"''\"):\n",
    "        return text\n",
    "    elif (text.isupper() and len(text)>2):\n",
    "        return \"NNP\"\n",
    "    elif (text[:1]).isupper() and len(text)>1 and to_lower(text[:2])!=\"wh\" and to_lower(text[:2])!=\"th\":\n",
    "        return \"NNP\"\n",
    "    else:\n",
    "        return \"<NONE>\"\n",
    "\n",
    "\n",
    "\n",
    "# Tags content with POS for new content \n",
    "# Handles unknown words\n",
    "def tag_pos_content(content, word_tag_max, IGNORE_):\n",
    "    count_total = 0\n",
    "    count_un = 0\n",
    "    count_tagged = 0\n",
    "    count_tagged_un = 0\n",
    "    count_not_tagged_un = 0\n",
    "\n",
    "    content_modified = \"\"\n",
    "    print(\"-------------------------Some taggging for new text shown below--------------------\")\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    for line in content:\n",
    "        line = line.strip()\n",
    "        words = line.split(\" \")\n",
    "\n",
    "        if line not in IGNORE_:\n",
    "            pos = 0\n",
    "            for word in words:\n",
    "                word = word.strip()\n",
    "                if word in word_tag_max:\n",
    "                    content_modified = content_modified + word_tag_max[word] + \" \" + word + \" \"\n",
    "                    count_tagged +=1\n",
    "                    if pos % 20 == 0:\n",
    "                        print(\"Word (Known-Tagged)      : %5s   %s\"% (word_tag_max[word], word))\n",
    "                else:\n",
    "                    tag = tag_unknown_words(line, pos, word, word_tag_max)\n",
    "                    \n",
    "                    if tag != \"<NONE>\":\n",
    "                        content_modified = content_modified + tag + \" \" + word + \" \"\n",
    "                        if pos % 10 == 0:\n",
    "                            print(\"Word (New-Tagged)        : %5s   %s\"% (tag, word))\n",
    "                        count_tagged_un +=1\n",
    "                    else:\n",
    "                        if pos % 10 == 0:\n",
    "                            print(\"Word (New-Not Tagged)    : %5s  %s\"% (tag, word))\n",
    "                        content_modified = content_modified + \"<NONE>\" + \" \" + word + \" \"\n",
    "                        count_not_tagged_un +=1\n",
    "                    count_un +=1\n",
    "\n",
    "                count_total +=1\n",
    "                pos +=1\n",
    "            content_modified = content_modified + \"\\n\"\n",
    "        else:\n",
    "            pass\n",
    "    print(\" _____________________________________________________________________________________________\")\n",
    "    return content_modified, count_total, count_un, count_tagged, count_tagged_un, count_not_tagged_un\n",
    "\n",
    "\n",
    "# main baseline_lexical_tagger for new file\n",
    "def tagger_unknown_corpus(word_tag_max, test_article, test_article_prep, test_article_tag, IGNORE_):\n",
    "    filename, content = clean_file(test_article)\n",
    "    content = preprocess_content(content, IGNORE_)\n",
    "    write(test_article_prep, content)\n",
    "    content = read_file_line_by_line(test_article_prep)\n",
    "    content_modified, count_total, count_un, count_tagged, count_tagged_un, count_not_tagged_un = tag_pos_content(content, word_tag_max, IGNORE_)\n",
    "    write(test_article_tag, content_modified)\n",
    "    return content_modified, count_total, count_un, count_tagged, count_tagged_un, count_not_tagged_un\n",
    "\n",
    "# baseline lexical tagger performance for new content\n",
    "def tagger_unknown_corpus_performance(content_modified, count_total, count_un, count_tagged, count_tagged_un, count_not_tagged_un):\t\n",
    "    count_total = float(count_total)\n",
    "    count_un = float(count_un)\n",
    "    count_tagged = float(count_tagged)\n",
    "    count_tagged_un = float(count_tagged_un)\n",
    "    count_not_tagged_un = float(count_not_tagged_un)\n",
    "\n",
    "\n",
    "    count_un_prct = (count_un/count_total)*100\n",
    "    count_tagged_prct = (count_tagged/count_total)*100\n",
    "    count_tagged_un_prct = (count_tagged_un/count_un)*100\n",
    "    count_not_tagged_un_prct = (count_not_tagged_un/count_un)*100\n",
    "    print(\"\\n______________________________________________Begin_____________________________________________________\\n\")\n",
    "    print(\"\\n                           Performance Report of New Content                                              \")\n",
    "    print(\" ___________________________________________________________________________________________________________\")\n",
    "    print(\" *********************************************************************************************************\\n\")\n",
    "    print(\" Total Number of Words                          : %s         \" % (int(count_total)))\n",
    "    print(\" Tagged Words Known (percentile among all words): %s (%.2f%s)\" % (int(count_tagged), count_tagged_prct, \"%\"))\n",
    "    print(\" New Words(percentile among all words)          : %s (%.2f%s)\" % (int(count_un), count_un_prct, \"%\"))\n",
    "    print(\" Words Tagged(percentile among all new words)   : %s (%.2f%s)\" % (int(count_tagged_un), count_tagged_un_prct, \"%\"))\n",
    "    print(\" Words Could Not Tag (percentile in new words)  : %s (%.2f%s)\" %(int(count_not_tagged_un), count_not_tagged_un_prct, \"%\"))\n",
    "    print(\" *********************************************************************************************************\")\n",
    "    print(\"\\n ______________________________________________END_____________________________________________________\\n\\n\\n\")\n",
    "\n",
    "\n",
    "#Assignment 3\n",
    "input_file = \"data/BROWN.pos.all\"\n",
    "output_file = \"out/BROWN-clean.pos.txt\"\n",
    "\n",
    "test_file = \"data/SnapshotBROWN.pos.all.txt\"\n",
    "test_file_out = \"out/SnapshotBROWN-clean.pos.txt\"\n",
    "\n",
    "test_article = \"data/article.txt\"\n",
    "test_article_prep = \"out/article_prep.txt\"\n",
    "test_article_tag = \"out/article_tag.txt\"\n",
    "\n",
    "IGNORE_ = [\"\", \"\\n\",\"\\r\", \"\\r\\n\", \"\\n\\r\"]\n",
    "\n",
    "word_tag_max = baseline_lexical_tagger(input_file, output_file, test_file, test_file_out)\n",
    "content, count_total, count_un, count_tagged, count_tagged_un, count_not_tagged_un = tagger_unknown_corpus(word_tag_max, test_article, test_article_prep, test_article_tag, IGNORE_)\n",
    "tagger_unknown_corpus_performance(content, count_total, count_un, count_tagged, count_tagged_un, count_not_tagged_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
